{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval with Text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [How Google (probably) implemented image search](https://www.quora.com/How-does-a-Google-image-search-engine-work/answer/John-Koala)\n",
    " * pattern matching to image metadata\n",
    " * content-based search for new images to identify tags\n",
    " * DL augmentations: add tags of recognised objects to an image, e.g. \"dog\", \"cat\", \"sky\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Recent Advance in Content-based Image Retrieval: A Literature Survey](https://arxiv.org/pdf/1706.06064.pdf)\n",
    " * TODO: classic inverted file structure to index large scale visual database for scalable retrieval\n",
    "* [Deep Image Retrieval: Learning global representations for image search](https://arxiv.org/pdf/1604.01325v2.pdf) - discovering concise fixed-lenght global image descriptor for CBIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potentially Relevant Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [A Case for Query by Image and Text Content: Searching Computer Help using Screenshots and Keywords](http://ramb.ethz.ch/CDstore/www2011/proceedings/p775.pdf) - investigate how to perform CBIR with a supporting text query\n",
    " * might not be very relevant since it seems to do it separately, i.e. first filter everything by keyword, and then filter results by image\n",
    "* [Web Image Re-Ranking Using Query-Specific Semantic Signatures](http://www.ee.cuhk.edu.hk/~xgwang/papers/wangQLTpami13.pdf) - proposes improvements to re-ranking images in a two-stage search: I) query images by keywords II) CBIS with visual feedback from user (i.e. google search -> show similar pictures)\n",
    "* [CBIR using Speech, Text & Image Query for Mobile Device](https://pdfs.semanticscholar.org/d176/435aedc195539765afe3e42da920758bdd34.pdf) - has wide literature overview for image search with text queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A New Benchmark and Approach for Fine-grained Cross-media Retrieval](https://arxiv.org/pdf/1907.04476.pdf)\n",
    "> Therefore,\n",
    "in this paper, we first construct a new benchmark for fine-grained\n",
    "cross-media retrieval, which consists of 200 fine-grained subcategories of the “Bird”, and contains 4 media types, including image,\n",
    "text, video and audio.\n",
    "\n",
    "> we propose a uniform deep model, namely FGCrossNet, which simultaneously learns 4 types of media without discriminative treatments\n",
    "\n",
    "While current reasearch has some progress in coarse-grained multimodal medai retrieval, this paper want to discover fine-coarced retrieval. That is, when you query with an image of \"Herring Gull\", they want to give you multimedai results with that particular species instead of anything related to a class \"BIRD\"\n",
    "\n",
    "> Fine-grained cross-media retrieval has three challenges: (1) Few datasets ... (2) Heterogeneity gap - Variant types of media have inconsistent distributions and feature representations ... (3) Small inter-class variance ...\n",
    "\n",
    "> , Peng et al. construct the PKU\n",
    "XMediaNet [1], which is the largest cross-media dataset with up to\n",
    "5 media types, including image, text, video, audio and 3D model. It\n",
    "contains 100,000 samples from 200 coarse-grained categories. Its\n",
    "categories are selected from WordNet 1\n",
    ", and cover 47 species of\n",
    "animals such as “Bird” and “Dog”, as well as 153 types of artifacts,\n",
    "such as “Airplane” and “Car”\n",
    "\n",
    "> Existing cross-media retrieval methods generally deal with different\n",
    "media data through different network streams, which causes some\n",
    "issues: (1) Architecture complexity - Different media data maybe\n",
    "processed by different types of networks... (thus when you merge all of them, it gets really complicated) (2) Training difficulty -\n",
    "Since the network architecture is complex, its training certainly\n",
    "will be difficult, which causes it hard to reproduce the method... we propose a uniform deep model, which adopts the\n",
    "same architecture to simultaneously learn 4 media data without\n",
    "discriminative treatments\n",
    "\n",
    ">  Data Preprocessing ... For image, there is no\n",
    "need to do any preprocessing. For video, we draw 25 uniformlyspaced frames of each video as the video data. For audio, we apply\n",
    "Short-Time Fourier Transformation [16] to generate spectrogram\n",
    "for each audio instance... {for description, i.e. text input, they do one-hot encoding of each character, producing 448x16 vector. Then apply a few convolutions and get uniformed format for all types of data input, i.e. 448x448x3 tensor)\n",
    "\n",
    "> loss function ...{consists of}... classification constraint ensures the learning of discriminative features for fine-grained subcategories, center constraint ensures the compactness characteristic of the features of\n",
    "the same subcategory, and ranking constraint ensures the sparsity\n",
    "characteristic of the features of different subcategories.\n",
    "\n",
    "*classification constraint* is just a sum of cross-entropy loss functions(i.e. loss between FC vector & label for that item) summed through all test samples and all types of input\n",
    "\n",
    "*center constraint* - suppose to minimize the intra-class variance. It's a squared sum of difference between i-th item features (FC vector, which is the result of NN) and respective label center(gets updated after each batch is processed and reassigned to different classes). The same approach for any media type\n",
    "\n",
    "*ranking constraint* - suppose to maximize the inter-class variance. It's a quadruplet loss function, which is basically a sum of squread differences between 4 data points. Also, those data points have following restrictions: 1) each of them is from different media type 2) they represent 3 different classes, i.e. 2 items are from the same class. Then the error = sum of distances is designed in a way to apply penalty for small intercluster distances as well as for big inner-cluster difference\n",
    "\n",
    "Training is done in a following way:\n",
    "* just as in ranking constraint, they take 4 instances of 4 different types but only from 3 categories at a time\n",
    "* firstly they fine-tune the NN trained on ImageNet only with their images, and only then start training with 4-items batches of different types\n",
    "\n",
    "TODO: finish the papaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
