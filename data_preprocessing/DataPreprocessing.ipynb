{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Articles Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** you can reproduce those results on Kaggle without downloading anything https://www.kaggle.com/jacksoncrow/dataset-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#!/bin/python3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join, exists, abspath\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import ResNet152, preprocess_input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading&Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _globalMaxPool1D(tensor):\n",
    "    _,_,_,size = tensor.shape\n",
    "    return [tensor[:,:,:,i].max() for i in range(size)]\n",
    "\n",
    "def _getImageFeatures(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=None)\n",
    "\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    feature_tensor = model.predict(img_data)\n",
    "    get_img_id = lambda p: p.split('/')[-1].split('.')[0]\n",
    "    return {\n",
    "        \"id\": get_img_id(img_path),\n",
    "        \"features\": _globalMaxPool1D(feature_tensor),\n",
    "    }\n",
    "\n",
    "def _getJSON(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.loads(json.load(json_file))\n",
    "    \n",
    "def _clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # onyshchak: only checking first 1000 characters, will need to extract summary propely\n",
    "    text = text[:1000].rsplit(' ', 1)[0]\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def _getTextFeatures(text_path):\n",
    "    data = _getJSON(text_path)\n",
    "    \n",
    "    return {\n",
    "        'id': data['id'],\n",
    "        'text': _clean_text(data['text']),\n",
    "        'title': data['title']\n",
    "    }\n",
    "\n",
    "def _getImagesMeta(path):\n",
    "    data = _getJSON(path)['img_meta']\n",
    "    for x in data:\n",
    "        x['description'] = _clean_text(x['description'])\n",
    "        x['title'] = _clean_text(x['title'])\n",
    "    return data\n",
    "\n",
    "def _getValidImagePaths(article_path):\n",
    "    img_path = join(article_path, 'img/')\n",
    "    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n",
    "\n",
    "def _dump(path, data):\n",
    "    with open(path, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(data, outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "def GetArticleData(article_path):\n",
    "    article_data = _getTextFeatures(join(article_path, 'text.json'))\n",
    "    article_data[\"img\"] = _getImagesMeta(join(article_path, 'img/', 'meta.json'))\n",
    "    \n",
    "    return article_data\n",
    "\n",
    "def ReadArticles(data_path, pred=None, offset=0, limit=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) - offset\n",
    "    limit = min(limit, len(article_paths) - offset)\n",
    "    \n",
    "    articles = []\n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        if (i - offset) % 300 == 0: print(i - offset, \"articles have been read\")\n",
    "        article_data = GetArticleData(path)\n",
    "        if pred and not pred(i, article_data): continue\n",
    "        \n",
    "        articles.append(article_data)\n",
    "        if len(articles) >= limit: break  # useless?\n",
    "        \n",
    "    print(offset + limit, \"articles have been read\")\n",
    "    return articles\n",
    "\n",
    "def GenerateVisualFeatures(data_path, offset=0, limit=None, model=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) - offset\n",
    "    limit = min(limit, len(article_paths) - offset)\n",
    "    model = model if model else ResNet152(weights='imagenet', include_top=False) \n",
    "    \n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        print(i, path)\n",
    "    \n",
    "        meta_path = join(path, 'img/', 'meta.json')\n",
    "        meta_arr = _getImagesMeta(meta_path)\n",
    "        for meta in meta_arr:\n",
    "            if 'features' in meta: continue\n",
    "            if meta['filename'][-4:].lower() != \".jpg\": continue\n",
    "                \n",
    "            img_path =  join(path, 'img/', meta['filename'])\n",
    "            try:\n",
    "                features = _getImageFeatures(model, img_path)['features']\n",
    "                meta['features'] = [str(f) for f in features]\n",
    "            except Exception as e:\n",
    "                print(\"exception\", str(e))\n",
    "                print(img_path)\n",
    "                continue\n",
    "                \n",
    "        _dump(meta_path, json.dumps({\"img_meta\": meta_arr}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet152(weights='imagenet', include_top=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# GenerateVisualFeatures(dataset_root, model=model, offset=0, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "articles = ReadArticles(dataset_root, offset=0, limit=None)\n",
    "processed_titles = _getJSON(\"parsed_titles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Preprocessed Dataset into W2VV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'data_w2vv'\n",
    "dataset_path = join('./', dataset_name)\n",
    "if exists(dataset_path):\n",
    "    shutil.rmtree(dataset_path)\n",
    "    \n",
    "mkdir(dataset_path)\n",
    "subsets = {\n",
    "    \"train\": {},\n",
    "    \"val\": {},\n",
    "    \"test\": {},\n",
    "}\n",
    "\n",
    "for k, v in subsets.items():\n",
    "    v['name'] = dataset_name + k\n",
    "    v['path'] = join(dataset_path, v['name'])\n",
    "    mkdir(v['path'])\n",
    "    \n",
    "    v['feature_data_path'] = join(v['path'], 'FeatureData')\n",
    "    if k == 'train':\n",
    "        mkdir(v['feature_data_path'])\n",
    "    else:\n",
    "        dst = v['feature_data_path']\n",
    "        os.symlink(os.path.relpath(subsets['train']['feature_data_path'], Path(dst).parent), dst)\n",
    "\n",
    "    v[\"image_sets_path\"] = join(v['path'], 'ImageSets')\n",
    "    mkdir(v[\"image_sets_path\"])\n",
    "\n",
    "    v[\"text_data_path\"] = join(v['path'], 'TextData')\n",
    "    mkdir(v[\"text_data_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(arr, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for x in arr:\n",
    "            f.write(\"%s\\n\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_img = lambda i: 'features' in i\n",
    "has_valid_img = lambda a: len([i for i in a['img'] if is_valid_img(i)]) > 0\n",
    "articles = [a for a in articles if has_valid_img(a)]\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2str = lambda l: \" \".join([str(x) for x in l])\n",
    "get_img_id = lambda i: os.path.splitext(i['filename'])[0]  # removing file extention\n",
    "\n",
    "img_features = set(['{} {}'.format(get_img_id(i), list2str(i['features'])) for a in articles for i in a['img'] if is_valid_img(i)])\n",
    "print(\"len(img_features) = \", len(img_features))\n",
    "\n",
    "raw_features_file_path = join(subsets['train'][\"feature_data_path\"], subsets['train']['name'] + \".features.txt\")\n",
    "to_file(img_features, raw_features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(articles):\n",
    "    seen = set()\n",
    "    res = []\n",
    "    for a in articles:\n",
    "        for i in a['img']:\n",
    "            if 'features' not in i: continue\n",
    "                \n",
    "            img_id = os.path.splitext(i['filename'])[0]  # removing file extention\n",
    "            if img_id in seen:\n",
    "                # onyshchak: if image used in 2 articles, we only take the first one for simplicity\n",
    "                # TODO: use all the infomation without breaking the model\n",
    "                continue\n",
    "                \n",
    "            seen.add(img_id)\n",
    "            res.append({\n",
    "                \"filename\": img_id,\n",
    "                'article_id': a['id'],\n",
    "                'article_title': a['title'],\n",
    "                \"title\": os.path.splitext(i['title'])[0],\n",
    "                \"description\": i['description'],\n",
    "                \"text\": a['text'],\n",
    "                \"features\": i['features'],\n",
    "            })\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_LEVEL_SPLIT = True\n",
    "seed = 1234\n",
    "\n",
    "if ARTICLE_LEVEL_SPLIT:\n",
    "    train, test = train_test_split(articles, test_size=0.04, random_state=seed)\n",
    "    train, val = train_test_split(train, test_size=0.043, random_state=seed)\n",
    "\n",
    "    subsets['train']['data'] = map_data(train)\n",
    "    subsets['val']['data'] = map_data(val)\n",
    "    subsets['test']['data'] = map_data(test)\n",
    "else:\n",
    "    mapped_images = map_data(articles)\n",
    "    # 2325 test, 2057 val\n",
    "    train, test = train_test_split(mapped_images, test_size=1000, random_state=seed)\n",
    "    train, val = train_test_split(train, test_size=1000, random_state=seed)\n",
    "    \n",
    "    subsets['train']['data'] = train\n",
    "    subsets['val']['data'] = val\n",
    "    subsets['test']['data'] = test\n",
    "\n",
    "subset_split_info = str([(k, len(v['data'])) for k, v in subsets.items()]).replace(\"(\", \"\").replace(\")\", \"\")\n",
    "subset_split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [x['article_id'] for x in subsets['test']['data']]\n",
    "to_file(ids, join(subsets['test']['path'], \"test_articles_ids.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in subsets.values():\n",
    "    ids = [x['filename'] for x in v['data']]\n",
    "    to_file(ids, join(v[\"image_sets_path\"], v['name'] + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# onyshchak: originally ID also contained file extention e.g. *.jpg. but not in image_sets_path\n",
    "def get_description(z):\n",
    "    if z['description']: return z['description']\n",
    "    elif z['filename'] in processed_titles: return processed_titles[z['filename']]\n",
    "    else: \n",
    "        print(\"Missing title\", z['filename'])\n",
    "        return z['title']\n",
    "\n",
    "for v in subsets.values():\n",
    "    text_data = sorted(\n",
    "        ['{}#enc#0 {}'.format(x['filename'], x['text']) for x in v['data']] +\n",
    "        ['{}#enc#1 {}'.format(x['filename'], get_description(x)) for x in v['data']]\n",
    "    )\n",
    "\n",
    "    to_file(text_data, join(v[\"text_data_path\"], v['name'] + \".caption.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del articles\n",
    "del text_data\n",
    "del train\n",
    "del val\n",
    "del ids\n",
    "del test\n",
    "del img_features\n",
    "for k,v in subsets.items():\n",
    "    del v['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_FILE_LIST = 0\n",
    "FEATURE_DIMENTION = 2048\n",
    "feature_data_path = subsets['train'][\"feature_data_path\"]\n",
    "bin_features_path = join(feature_data_path, \"pyresnet152-pool5os/\")\n",
    "\n",
    "! python2 /kaggle/input/w2vv-scripts/simpleknn/txt2bin.py $FEATURE_DIMENTION $raw_features_file_path $IS_FILE_LIST $bin_features_path --overwrite 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
