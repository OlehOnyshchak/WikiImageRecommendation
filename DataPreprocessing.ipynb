{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Articles Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#!/bin/python3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join, exists, abspath\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import ResNet152, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading&Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _globalMaxPool1D(tensor):\n",
    "    _,_,_,size = tensor.shape\n",
    "    return [tensor[:,:,:,i].max() for i in range(size)]\n",
    "\n",
    "def _getImageFeatures(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=None)\n",
    "\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    feature_tensor = model.predict(img_data)\n",
    "    get_img_id = lambda p: p.split('/')[-1].split('.')[0]\n",
    "    return {\n",
    "        \"id\": get_img_id(img_path),\n",
    "        \"features\": _globalMaxPool1D(feature_tensor),\n",
    "    }\n",
    "\n",
    "def _getTextFeatures(text_path):\n",
    "    with open(text_path) as json_file:\n",
    "        data = json.loads(json.load(json_file))\n",
    "        text = data['text'].replace(\"\\n\", \" \")\n",
    "        return {\n",
    "            'id': data['id'],\n",
    "            'text': text.translate(str.maketrans('', '', string.punctuation)),\n",
    "        }\n",
    "    \n",
    "def _getValidImagePaths(article_path):\n",
    "    img_path = join(article_path, 'img/')\n",
    "    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n",
    "\n",
    "def GetArticleData(model, article_path):\n",
    "    article_data = _getTextFeatures(join(article_path, 'text.json'))\n",
    "    article_data[\"img\"] = []\n",
    "    for img_path in _getValidImagePaths(article_path):\n",
    "        img_features = _getImageFeatures(model, img_path)\n",
    "        article_data[\"img\"].append(img_features)\n",
    "        \n",
    "    return article_data\n",
    "\n",
    "def PreprocessArticles(data_path, limit=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) + 1\n",
    "    model = ResNet152(weights='imagenet', include_top=False) \n",
    "    \n",
    "    articles = []\n",
    "    for path in article_paths:\n",
    "        article_data = GetArticleData(model, path)\n",
    "        articles.append(article_data)\n",
    "        if len(articles) >= limit: break\n",
    "            \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 29s, sys: 2min 38s, total: 7min 7s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles = PreprocessArticles('./data/', limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Preprocessed Dataset into W2VV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = 'test_subset'\n",
    "subset_path = abspath(join('./', subset_name))\n",
    "if exists(subset_path):\n",
    "    shutil.rmtree(subset_path)\n",
    "    \n",
    "mkdir(subset_path)\n",
    "\n",
    "feature_data_path = join(subset_path, 'FeatureData')\n",
    "mkdir(feature_data_path)\n",
    "\n",
    "image_sets_path = join(subset_path, 'ImageSets')\n",
    "mkdir(image_sets_path)\n",
    "\n",
    "text_data_path = join(subset_path, 'TextData')\n",
    "mkdir(text_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(arr, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for x in arr:\n",
    "            f.write(\"%s\\n\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [i['id'] for a in articles for i in a['img']]\n",
    "to_file(ids, join(image_sets_path, subset_name + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['{}#enc#0 {}'.format(i['id'], a['text']) for a in articles for i in a['img']]\n",
    "to_file(text_data, join(text_data_path, subset_name + \".caption.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list2str = lambda l: \" \".join([str(x) for x in l])\n",
    "img_features = ['{} {}'.format(i['id'], list2str(i['features'])) for a in articles for i in a['img']]\n",
    "\n",
    "raw_features_file_path = join(feature_data_path, subset_name + \".features.txt\")\n",
    "to_file(img_features, raw_features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Processing /home/oleh/projects/WikiImageRecommendation/test_subset/FeatureData/test_subset.features.txt\r\n",
      "17 lines parsed, 17 ids,  0 failed ->  17 unique ids\r\n"
     ]
    }
   ],
   "source": [
    "IS_FILE_LIST = 0\n",
    "FEATURE_DIMENTION = 2048\n",
    "bin_features_path = join(feature_data_path, \"pyresnet152-pool5os/\")\n",
    "\n",
    "! python2 w2vv/simpleknn/txt2bin.py $FEATURE_DIMENTION $raw_features_file_path $IS_FILE_LIST $bin_features_path --overwrite 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
