{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Articles Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** you can reproduce those results on Kaggle without downloading anything https://www.kaggle.com/jacksoncrow/dataset-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#!/bin/python3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join, exists, abspath\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import ResNet152, preprocess_input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading&Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _globalMaxPool1D(tensor):\n",
    "    _,_,_,size = tensor.shape\n",
    "    return [tensor[:,:,:,i].max() for i in range(size)]\n",
    "\n",
    "def _getImageFeatures(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=None)\n",
    "\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    feature_tensor = model.predict(img_data)\n",
    "    get_img_id = lambda p: p.split('/')[-1].split('.')[0]\n",
    "    return {\n",
    "        \"id\": get_img_id(img_path),\n",
    "        \"features\": _globalMaxPool1D(feature_tensor),\n",
    "    }\n",
    "\n",
    "def _getJSON(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.loads(json.load(json_file))\n",
    "\n",
    "def _getTextFeatures(text_path):\n",
    "    data = _getJSON(text_path)\n",
    "    text = data['text'].replace(\"\\n\", \" \")\n",
    "    # onyshchak: only checking first 1000 characters, will need to extract summary propely\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))[:1000]\n",
    "    return {\n",
    "        'id': data['id'],\n",
    "        'text': text,\n",
    "    }\n",
    "\n",
    "def _getImagesMeta(path):\n",
    "    return _getJSON(path)['img_meta']\n",
    "\n",
    "def _getValidImagePaths(article_path):\n",
    "    img_path = join(article_path, 'img/')\n",
    "    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n",
    "\n",
    "def _dump(path, data):\n",
    "    with open(path, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(data, outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "def GetArticleData(article_path):\n",
    "    article_data = _getTextFeatures(join(article_path, 'text.json'))\n",
    "    article_data[\"img\"] = _getImagesMeta(join(article_path, 'img/', 'meta.json'))\n",
    "    \n",
    "    return article_data\n",
    "\n",
    "def ReadArticles(data_path, offset=0, limit=None):\n",
    "    print(\"Reading in progress...\")\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) - offset\n",
    "    \n",
    "    articles = []\n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        if (i - offset + 1) % 251 == 0: print(i - offset, \"articles have been read\")\n",
    "        article_data = GetArticleData(path)\n",
    "        articles.append(article_data)\n",
    "        if len(articles) >= limit: break  # useless?\n",
    "        \n",
    "    print(limit, \"articles have been read\")\n",
    "    return articles\n",
    "\n",
    "def GenerateVisualFeatures(data_path, offset=0, limit=None, model=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) - offset\n",
    "    model = model if model else ResNet152(weights='imagenet', include_top=False) \n",
    "    \n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        print(i, path)\n",
    "    \n",
    "        meta_path = join(path, 'img/', 'meta.json')\n",
    "        meta_arr = _getImagesMeta(meta_path)\n",
    "        for meta in meta_arr:\n",
    "            if 'features' in meta: continue\n",
    "            if meta['filename'][-4:].lower() != \".jpg\": continue\n",
    "                \n",
    "            img_path =  join(path, 'img/', meta['filename'])\n",
    "            try:\n",
    "                features = _getImageFeatures(model, img_path)['features']\n",
    "                meta['features'] = [str(f) for f in features]\n",
    "            except Exception as e:\n",
    "                print(\"exception\", str(e))\n",
    "                print(img_path)\n",
    "                continue\n",
    "                \n",
    "        _dump(meta_path, json.dumps({\"img_meta\": meta_arr}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "GenerateVisualFeatures('./data/', offset=0, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "articles = ReadArticles('./data/', offset=0, limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Preprocessed Dataset into W2VV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'data_w2vv'\n",
    "dataset_path = join('./', dataset_name)\n",
    "if exists(dataset_path):\n",
    "    shutil.rmtree(dataset_path)\n",
    "    \n",
    "mkdir(dataset_path)\n",
    "subsets = {\n",
    "    \"train\": {},\n",
    "    \"val\": {},\n",
    "    \"test\": {},\n",
    "}\n",
    "\n",
    "for k, v in subsets.items():\n",
    "    v['name'] = dataset_name + k\n",
    "    v['path'] = join(dataset_path, v['name'])\n",
    "    mkdir(v['path'])\n",
    "    \n",
    "    v['feature_data_path'] = join(v['path'], 'FeatureData')\n",
    "    if k == 'train':\n",
    "        mkdir(v['feature_data_path'])\n",
    "    else:\n",
    "        dst = v['feature_data_path']\n",
    "        os.symlink(os.path.relpath(subsets['train']['feature_data_path'], Path(dst).parent), dst)\n",
    "\n",
    "    v[\"image_sets_path\"] = join(v['path'], 'ImageSets')\n",
    "    mkdir(v[\"image_sets_path\"])\n",
    "\n",
    "    v[\"text_data_path\"] = join(v['path'], 'TextData')\n",
    "    mkdir(v[\"text_data_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(arr, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for x in arr:\n",
    "            f.write(\"%s\\n\" % x)\n",
    "            \n",
    "# map_data = lambda func: [func(a, i) for a in articles for i in a['img'] if 'features' in i]\n",
    "def map_data():\n",
    "    seen = set()\n",
    "    res = []\n",
    "    for a in articles:\n",
    "        for i in a['img']:\n",
    "            if 'features' not in i: continue\n",
    "                \n",
    "            img_id = os.path.splitext(i['filename'])[0]  # removing file extention\n",
    "            if img_id in seen:\n",
    "                # onyshchak: if image used in 2 articles, we only take the first one for simplicity\n",
    "                # TODO: use all the infomation without breaking the model\n",
    "                continue\n",
    "                \n",
    "            seen.add(img_id)\n",
    "            res.append({\n",
    "                \"filename\": img_id,\n",
    "                \"title\": i['title'],\n",
    "                \"text\": a['text'][:1000],\n",
    "                \"features\": i['features'],\n",
    "            })\n",
    "            \n",
    "    return res\n",
    "\n",
    "data = map_data()\n",
    "del articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2str = lambda l: \" \".join([str(x) for x in l])\n",
    "\n",
    "img_features = ['{} {}'.format(x['filename'], list2str(x['features'])) for x in data]\n",
    "\n",
    "raw_features_file_path = join(subsets['train'][\"feature_data_path\"], subsets['train']['name'] + \".features.txt\")\n",
    "to_file(img_features, raw_features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets['train']['data'], subsets['test']['data'] = train_test_split(\n",
    "    data, test_size=1000, random_state=1234\n",
    ")\n",
    "\n",
    "subsets['train']['data'], subsets['val']['data'] = train_test_split(\n",
    "    subsets['train']['data'], test_size=1000, random_state=1234\n",
    ")\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in subsets.values():\n",
    "    ids = [x['filename'] for x in v['data']]\n",
    "    to_file(ids, join(v[\"image_sets_path\"], v['name'] + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onyshchak: originally ID also contained file extention e.g. *.jpg. but not in image_sets_path\n",
    "for v in subsets.values():\n",
    "    text_data = sorted(\n",
    "        ['{}#enc#0 {}'.format(x['filename'], x['title']) for x in v['data']] +\n",
    "        ['{}#enc#1 {}'.format(x['filename'], x['text'][:1000]) for x in v['data']]\n",
    "    )\n",
    "\n",
    "    to_file(text_data, join(v[\"text_data_path\"], v['name'] + \".caption.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in subsets.items():\n",
    "    del v['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Processing ./data_w2vv/data_w2vvtrain/FeatureData/data_w2vvtrain.features.txt\n",
      "45643 lines parsed, 45643 ids,  0 failed ->  45643 unique ids\n"
     ]
    }
   ],
   "source": [
    "IS_FILE_LIST = 0\n",
    "FEATURE_DIMENTION = 2048\n",
    "feature_data_path = subsets['train'][\"feature_data_path\"]\n",
    "bin_features_path = join(feature_data_path, \"pyresnet152-pool5os/\")\n",
    "\n",
    "! python2 w2vv/simpleknn/txt2bin.py $FEATURE_DIMENTION $raw_features_file_path $IS_FILE_LIST $bin_features_path --overwrite 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
