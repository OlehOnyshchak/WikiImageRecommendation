{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Articles Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#!/bin/python3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join, exists, abspath\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import ResNet152, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading&Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _globalMaxPool1D(tensor):\n",
    "    _,_,_,size = tensor.shape\n",
    "    return [tensor[:,:,:,i].max() for i in range(size)]\n",
    "\n",
    "def _getImageFeatures(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=None)\n",
    "\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    feature_tensor = model.predict(img_data)\n",
    "    get_img_id = lambda p: p.split('/')[-1].split('.')[0]\n",
    "    return {\n",
    "        \"id\": get_img_id(img_path),\n",
    "        \"features\": _globalMaxPool1D(feature_tensor),\n",
    "    }\n",
    "\n",
    "def _getJSON(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.loads(json.load(json_file))\n",
    "\n",
    "def _getTextFeatures(text_path):\n",
    "    data = _getJSON(text_path)\n",
    "    text = data['text'].replace(\"\\n\", \" \")\n",
    "    # onyshchak: only checking first 1000 characters, will need to extract summary propely\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))[:1000]\n",
    "    return {\n",
    "        'id': data['id'],\n",
    "        'text': text,\n",
    "    }\n",
    "\n",
    "def _getImagesMeta(path):\n",
    "    return _getJSON(path)['img_meta']\n",
    "\n",
    "def _getValidImagePaths(article_path):\n",
    "    img_path = join(article_path, 'img/')\n",
    "    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n",
    "\n",
    "def _dump(path, data):\n",
    "    with open(path, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(data, outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "def GetArticleData(model, article_path):\n",
    "    article_data = _getTextFeatures(join(article_path, 'text.json'))\n",
    "    article_data[\"img\"] = _getImagesMeta(join(article_path, 'img/', 'meta.json'))\n",
    "    \n",
    "    return article_data\n",
    "\n",
    "def ReadArticles(data_path, offset=0, limit=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths)\n",
    "    \n",
    "    articles = []\n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        print(i, path)\n",
    "        article_data = GetArticleData(model, path)\n",
    "        articles.append(article_data)\n",
    "        if len(articles) >= limit: break  # useless?\n",
    "            \n",
    "    return articles\n",
    "\n",
    "def GenerateVisualFeatures(data_path, offset=0, limit=None, model=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) - offset\n",
    "    model = model if model else ResNet152(weights='imagenet', include_top=False) \n",
    "    \n",
    "    articles = []\n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        print(i, path)\n",
    "    \n",
    "        meta_path = join(path, 'img/', 'meta.json')\n",
    "        meta_arr = _getImagesMeta(meta_path)\n",
    "        for meta in meta_arr:\n",
    "            if 'features' in meta: continue\n",
    "            if meta['filename'][-4:].lower() != \".jpg\": continue\n",
    "                \n",
    "            img_path =  join(path, 'img/', meta['filename'])\n",
    "            try:\n",
    "                features = _getImageFeatures(model, img_path)['features']\n",
    "                meta['features'] = [str(f) for f in features]\n",
    "            except Exception as e:\n",
    "                print(\"exception\", str(e))\n",
    "                print(img_path)\n",
    "                continue\n",
    "                \n",
    "        _dump(meta_path, json.dumps({\"img_meta\": meta_arr}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet152(weights='imagenet', include_top=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "GenerateVisualFeatures('./data/', offset=0, limit=None, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "articles = ReadArticles('./data/', offset=0, limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Preprocessed Dataset into W2VV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = 'test_subset3'\n",
    "subset_path = abspath(join('./', subset_name))\n",
    "if exists(subset_path):\n",
    "    shutil.rmtree(subset_path)\n",
    "    \n",
    "mkdir(subset_path)\n",
    "\n",
    "feature_data_path = join(subset_path, 'FeatureData')\n",
    "mkdir(feature_data_path)\n",
    "\n",
    "image_sets_path = join(subset_path, 'ImageSets')\n",
    "mkdir(image_sets_path)\n",
    "\n",
    "text_data_path = join(subset_path, 'TextData')\n",
    "mkdir(text_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(arr, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for x in arr:\n",
    "            f.write(\"%s\\n\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [i['id'] for a in articles for i in a['img']]\n",
    "to_file(ids, join(image_sets_path, subset_name + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onyshchak: originally ID also contained file extention e.g. \".jpg\"\n",
    "text_data = ['{}#enc#0 {}'.format(i['id'], i['title']) for a in articles for i in a['img']]\n",
    "to_file(text_data, join(text_data_path, subset_name + \".caption.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list2str = lambda l: \" \".join([str(x) for x in l])\n",
    "img_features = ['{} {}'.format(i['id'], list2str(i['features'])) for a in articles for i in a['img']]\n",
    "\n",
    "raw_features_file_path = join(feature_data_path, subset_name + \".features.txt\")\n",
    "to_file(img_features, raw_features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IS_FILE_LIST = 0\n",
    "FEATURE_DIMENTION = 2048\n",
    "bin_features_path = join(feature_data_path, \"pyresnet152-pool5os/\")\n",
    "\n",
    "# ! ./w2vv/do_gene_vocab.sh $subset_name # problems with relative path\n",
    "! python2 w2vv/simpleknn/txt2bin.py $FEATURE_DIMENTION $raw_features_file_path $IS_FILE_LIST $bin_features_path --overwrite 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
