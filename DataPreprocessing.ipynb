{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Articles Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#!/bin/python3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join, exists, abspath\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import ResNet152, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading&Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _globalMaxPool1D(tensor):\n",
    "    _,_,_,size = tensor.shape\n",
    "    return [tensor[:,:,:,i].max() for i in range(size)]\n",
    "\n",
    "def _getImagePixels(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=None)\n",
    "\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    feature_tensor = model.predict(img_data)\n",
    "    get_img_id = lambda p: p.split('/')[-1].split('.')[0]\n",
    "    return {\n",
    "        \"id\": get_img_id(img_path),\n",
    "        \"features\": _globalMaxPool1D(feature_tensor),\n",
    "    }\n",
    "\n",
    "def _getJSON(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.loads(json.load(json_file))\n",
    "\n",
    "def _getTextFeatures(text_path):\n",
    "    data = _getJSON(text_path)\n",
    "    text = data['text'].replace(\"\\n\", \" \")\n",
    "    # onyshchak: only checking first 1000 characters, will need to extract summery propely\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))[:1000]\n",
    "    return {\n",
    "        'id': data['id'],\n",
    "        'text': text,\n",
    "    }\n",
    "\n",
    "def _getImagesMeta(path):\n",
    "    return _getJSON(path)['img_meta']\n",
    "\n",
    "def _getValidImagePaths(article_path):\n",
    "    img_path = join(article_path, 'img/')\n",
    "    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n",
    "\n",
    "def _getMetaForImage(path, meta_arr):\n",
    "    filename = Path(path).name\n",
    "    return next(x for x in meta_arr if x['filename'] == filename)\n",
    "\n",
    "def GetArticleData(model, article_path):\n",
    "    article_data = _getTextFeatures(join(article_path, 'text.json'))\n",
    "    article_data[\"img\"] = []\n",
    "    meta_arr = _getImagesMeta(join(article_path, 'img/', 'meta.json'))\n",
    "    for img_path in _getValidImagePaths(article_path):\n",
    "        pixels = _getImagePixels(model, img_path)\n",
    "        meta = None\n",
    "        try:\n",
    "            meta = _getMetaForImage(img_path, meta_arr)\n",
    "        except Exception as e:\n",
    "            print(\"Exxception \", str(e))\n",
    "            print(\"ARTICLE \", article_path)\n",
    "            print(\"Image \", img_path)\n",
    "            print(\"META IMG \", [x['filename'] for x in meta_arr])\n",
    "            continue\n",
    "            raise\n",
    "            \n",
    "        img_features = {**pixels, **meta}\n",
    "        article_data[\"img\"].append(img_features)\n",
    "        \n",
    "    return article_data\n",
    "\n",
    "def PreprocessArticles(data_path, offset=0, limit=None):\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths)\n",
    "    model = ResNet152(weights='imagenet', include_top=False) \n",
    "    \n",
    "    articles = []\n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        print(i, path)\n",
    "        article_data = GetArticleData(model, path)\n",
    "        articles.append(article_data)\n",
    "        if len(articles) >= limit: break\n",
    "            \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "articles = PreprocessArticles('./data/', offset=0, limit=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "text_path = 'data/Emmeline_Pankhurst/img/meta.json'\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "with open(text_path) as json_file:\n",
    "    data = json.loads(json.load(json_file))\n",
    "    pp.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Preprocessed Dataset into W2VV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = 'test_subset3'\n",
    "subset_path = abspath(join('./', subset_name))\n",
    "if exists(subset_path):\n",
    "    shutil.rmtree(subset_path)\n",
    "    \n",
    "mkdir(subset_path)\n",
    "\n",
    "feature_data_path = join(subset_path, 'FeatureData')\n",
    "mkdir(feature_data_path)\n",
    "\n",
    "image_sets_path = join(subset_path, 'ImageSets')\n",
    "mkdir(image_sets_path)\n",
    "\n",
    "text_data_path = join(subset_path, 'TextData')\n",
    "mkdir(text_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(arr, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for x in arr:\n",
    "            f.write(\"%s\\n\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [i['id'] for a in articles for i in a['img']]\n",
    "to_file(ids, join(image_sets_path, subset_name + \".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onyshchak: originally ID also contained file extention e.g. \".jpg\"\n",
    "text_data = ['{}#enc#0 {}'.format(i['id'], i['title']) for a in articles for i in a['img']]\n",
    "to_file(text_data, join(text_data_path, subset_name + \".caption.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list2str = lambda l: \" \".join([str(x) for x in l])\n",
    "img_features = ['{} {}'.format(i['id'], list2str(i['features'])) for a in articles for i in a['img']]\n",
    "\n",
    "raw_features_file_path = join(feature_data_path, subset_name + \".features.txt\")\n",
    "to_file(img_features, raw_features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Processing /home/oleh/projects/WikiImageRecommendation/test_subset3/FeatureData/test_subset3.features.txt\n",
      "264 lines parsed, 263 ids,  0 failed ->  263 unique ids\n"
     ]
    }
   ],
   "source": [
    "IS_FILE_LIST = 0\n",
    "FEATURE_DIMENTION = 2048\n",
    "bin_features_path = join(feature_data_path, \"pyresnet152-pool5os/\")\n",
    "\n",
    "# ! ./w2vv/do_gene_vocab.sh $subset_name # problems with relative path\n",
    "! python2 w2vv/simpleknn/txt2bin.py $FEATURE_DIMENTION $raw_features_file_path $IS_FILE_LIST $bin_features_path --overwrite 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
